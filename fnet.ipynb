{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e959cded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 02:48:51.835990: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-13 02:48:51.949127: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-13 02:48:51.949143: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-13 02:48:51.974712: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-13 02:48:52.535259: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-13 02:48:52.535399: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-13 02:48:52.535409: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Defining hyperparameters\n",
    "\n",
    "VOCAB_SIZE = 8192\n",
    "MAX_SAMPLES = 50000\n",
    "BUFFER_SIZE = 20000\n",
    "MAX_LENGTH = 40\n",
    "EMBED_DIM = 256\n",
    "LATENT_DIM = 512\n",
    "NUM_HEADS = 8\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8537400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 02:48:53.480777: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-13 02:48:53.480814: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-13 02:48:53.480830: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2022-11-13 02:48:53.481012: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = keras.utils.get_file(\n",
    "    \"cornell_movie_dialogs.zip\",\n",
    "    origin=\"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "\n",
    "path_to_dataset = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\"\n",
    ")\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, \"movie_lines.txt\")\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset, \"movie_conversations.txt\")\n",
    "\n",
    "\n",
    "def load_conversations():\n",
    "    # Helper function for loading the conversation splits\n",
    "    id2line = {}\n",
    "    with open(path_to_movie_lines, errors=\"ignore\") as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace(\"\\n\", \"\").split(\" +++$+++ \")\n",
    "        id2line[parts[0]] = parts[4]\n",
    "\n",
    "    inputs, outputs = [], []\n",
    "    with open(path_to_movie_conversations, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace(\"\\n\", \"\").split(\" +++$+++ \")\n",
    "        # get conversation in a list of line ID\n",
    "        conversation = [line[1:-1] for line in parts[3][1:-1].split(\", \")]\n",
    "        for i in range(len(conversation) - 1):\n",
    "            inputs.append(id2line[conversation[i]])\n",
    "            outputs.append(id2line[conversation[i + 1]])\n",
    "            if len(inputs) >= MAX_SAMPLES:\n",
    "                return inputs, outputs\n",
    "    return inputs, outputs\n",
    "\n",
    "# load\n",
    "questions, answers = load_conversations()\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    sentence = tf.strings.lower(sentence)\n",
    "    # Adding a space between the punctuation and the last word to allow better tokenization\n",
    "    sentence = tf.strings.regex_replace(sentence, r\"([?.!,])\", r\" \\1 \")\n",
    "    # Replacing multiple continuous spaces with a single space\n",
    "    sentence = tf.strings.regex_replace(sentence, r\"\\s\\s+\", \" \")\n",
    "    # Replacing non english words with spaces\n",
    "    sentence = tf.strings.regex_replace(sentence, r\"[^a-z?.!,]+\", \" \")\n",
    "    sentence = tf.strings.strip(sentence)\n",
    "    sentence = tf.strings.join([\"[start]\", sentence, \"[end]\"], separator=\" \")\n",
    "    return sentence\n",
    "\n",
    "\n",
    "vectorizer = layers.TextVectorization(\n",
    "    VOCAB_SIZE,\n",
    "    standardize=preprocess_text,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LENGTH,\n",
    ")\n",
    "\n",
    "# We will adapt the vectorizer to both the questions and answers\n",
    "# This dataset is batched to parallelize and speed up the process\n",
    "vectorizer.adapt(tf.data.Dataset.from_tensor_slices((questions + answers)).batch(128))\n",
    "\n",
    "def vectorize_text(inputs, outputs):\n",
    "    inputs, outputs = vectorizer(inputs), vectorizer(outputs)\n",
    "    # One extra padding token to the right to match the output shape\n",
    "    outputs = tf.pad(outputs, [[0, 1]])\n",
    "    return (\n",
    "        {\"encoder_inputs\": inputs, \"decoder_inputs\": outputs[:-1]},\n",
    "        {\"outputs\": outputs[1:]},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a291c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee746ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, **kwargs):\n",
    "        super(FNetEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Casting the inputs to complex64\n",
    "        inp_complex = tf.cast(inputs, tf.complex64)\n",
    "        # Projecting the inputs to the frequency domain using FFT2D and\n",
    "        # extracting the real part of the output\n",
    "        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n",
    "        proj_input = self.layernorm_1(inputs + fft)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    \n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class FNetDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(FNetDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(latent_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    encoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"encoder_inputs\")\n",
    "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(encoder_inputs)\n",
    "    encoder_outputs = FNetEncoder(EMBED_DIM, LATENT_DIM)(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "    decoder_inputs = keras.Input(shape=(None,), dtype=\"int32\", name=\"decoder_inputs\")\n",
    "    encoded_seq_inputs = keras.Input(\n",
    "        shape=(None, EMBED_DIM), name=\"decoder_state_inputs\"\n",
    "    )\n",
    "    x = PositionalEmbedding(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM)(decoder_inputs)\n",
    "    x = FNetDecoder(EMBED_DIM, LATENT_DIM, NUM_HEADS)(x, encoded_seq_inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    decoder_outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "    decoder = keras.Model(\n",
    "        [decoder_inputs, encoded_seq_inputs], decoder_outputs, name=\"outputs\"\n",
    "    )\n",
    "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "    fnet = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"fnet\")\n",
    "    return fnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612a444",
   "metadata": {},
   "source": [
    "### Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b506b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 40000\n",
    "VALIDATION_SIZE = 5000\n",
    "TEST_SIZE = 5000\n",
    "\n",
    "# a number of total model = index 0 of locals is center model\n",
    "LOCAL_COUNT = 8\n",
    "DIVISION_COUNT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a8bd27",
   "metadata": {},
   "source": [
    "### Global Train/Validation Split\n",
    "###### Tokenizing and padding sentences using `TextVectorization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "055c62b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global\n",
    "# 0 ~ TRAIN_SIZE\n",
    "# TRAIN_SIZE + VALIDATION_SIZE ~ RAIN_SIZE + VALIDATION_SIZE + TEST_SIZE\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((questions[TRAIN_SIZE + VALIDATION_SIZE:TRAIN_SIZE + VALIDATION_SIZE + TEST_SIZE],\n",
    "                                                   answers[TRAIN_SIZE + VALIDATION_SIZE:TRAIN_SIZE + VALIDATION_SIZE + TEST_SIZE]))\n",
    "test_dataset = test_dataset.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b68f20",
   "metadata": {},
   "source": [
    "### FedAvg Train/Validation Split\n",
    "###### Tokenizing and padding sentences using `TextVectorization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf5bbe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FedAvg \n",
    "# local: 0, 1, 2 ... LOCAL_COUNT\n",
    "# fedavg_train_dataset[local]\n",
    "fedavg_train_dataset = []\n",
    "fedavg_validation_dataset = []\n",
    "\n",
    "# parameter\n",
    "train_size = int(TRAIN_SIZE / LOCAL_COUNT)\n",
    "validation_size = int(VALIDATION_SIZE / LOCAL_COUNT)\n",
    "\n",
    "for i in range(0, LOCAL_COUNT):\n",
    "    fedavg_train_dataset.append(\n",
    "        tf.data.Dataset.from_tensor_slices((questions[train_size * i:train_size * (i + 1)],\n",
    "                                            answers[train_size * i:train_size * (i + 1)]))\n",
    "     )\n",
    "    fedavg_train_dataset[i] = fedavg_train_dataset[i].map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    fedavg_train_dataset[i] = (\n",
    "        fedavg_train_dataset[i].cache()\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    fedavg_validation_dataset.append(\n",
    "        tf.data.Dataset.from_tensor_slices((questions[TRAIN_SIZE + validation_size * i:TRAIN_SIZE + validation_size * (i + 1)],\n",
    "                                            answers[TRAIN_SIZE + validation_size * i:TRAIN_SIZE + validation_size * (i + 1)]))\n",
    "    )\n",
    "    fedavg_validation_dataset[i] = fedavg_validation_dataset[i].map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    fedavg_validation_dataset[i] = fedavg_validation_dataset[i].cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027ae4d8",
   "metadata": {},
   "source": [
    "### FedAvg Accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f852d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 80s 989ms/step - loss: 2.0195 - accuracy: 0.1858 - val_loss: 1.6223 - val_accuracy: 0.2645\n",
      "79/79 [==============================] - 72s 887ms/step - loss: 2.0909 - accuracy: 0.1729 - val_loss: 1.6498 - val_accuracy: 0.2372\n",
      "79/79 [==============================] - 70s 858ms/step - loss: 1.8231 - accuracy: 0.2125 - val_loss: 1.9287 - val_accuracy: 0.2537\n",
      "79/79 [==============================] - 72s 876ms/step - loss: 2.0595 - accuracy: 0.1729 - val_loss: 1.5714 - val_accuracy: 0.2574\n",
      "79/79 [==============================] - 72s 891ms/step - loss: 2.0801 - accuracy: 0.1868 - val_loss: 1.6137 - val_accuracy: 0.2661\n",
      "79/79 [==============================] - 74s 903ms/step - loss: 1.9586 - accuracy: 0.1812 - val_loss: 1.6945 - val_accuracy: 0.2587\n",
      "79/79 [==============================] - 71s 875ms/step - loss: 1.9735 - accuracy: 0.1880 - val_loss: 1.8638 - val_accuracy: 0.2281\n",
      "79/79 [==============================] - 72s 887ms/step - loss: 2.0323 - accuracy: 0.1789 - val_loss: 1.7061 - val_accuracy: 0.2473\n",
      "\n",
      "\n",
      "\n",
      "79/79 [==============================] - 29s 363ms/step - loss: 2.8692 - accuracy: 0.2266\n",
      "test loss, test acc: [2.869209051132202, 0.22658059000968933]\n"
     ]
    }
   ],
   "source": [
    "fnet_fedavg = []\n",
    "weights_average = []\n",
    "\n",
    "for local in range(0, LOCAL_COUNT):\n",
    "    fnet_fedavg.append(create_model())\n",
    "    fnet_fedavg[local].compile(\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    fnet_fedavg[local].fit(fedavg_train_dataset[local], epochs=1, validation_data=fedavg_validation_dataset[local])\n",
    "    \n",
    "for i in range(0, len(fnet_fedavg[0].get_weights())):\n",
    "    avg = fnet_fedavg[0].get_weights()[i]\n",
    "    for local in range(1, LOCAL_COUNT):\n",
    "        avg += fnet_fedavg[local].get_weights()[i]\n",
    "    avg = avg / LOCAL_COUNT\n",
    "    weights_average.append(avg)\n",
    "\n",
    "for local in range(0, LOCAL_COUNT):\n",
    "    fnet_fedavg[local].set_weights(weights_average)\n",
    "\n",
    "print (\"\\n\\n\")\n",
    "results = fnet_fedavg[0].evaluate(test_dataset, batch_size=1)\n",
    "print (\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9eafe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ";constructor\n",
      ";destructor\n",
      "3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "./libc_module.so: undefined symbol: calc",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m res \u001b[38;5;241m=\u001b[39m add(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[0;32m---> 14\u001b[0m comm \u001b[38;5;241m=\u001b[39m \u001b[43mc_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc\u001b[49m\n\u001b[1;32m     15\u001b[0m comm\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     16\u001b[0m comm()\n",
      "File \u001b[0;32m/usr/lib/python3.8/ctypes/__init__.py:386\u001b[0m, in \u001b[0;36mCDLL.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(name)\n\u001b[0;32m--> 386\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, func)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "File \u001b[0;32m/usr/lib/python3.8/ctypes/__init__.py:391\u001b[0m, in \u001b[0;36mCDLL.__getitem__\u001b[0;34m(self, name_or_ordinal)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_ordinal):\n\u001b[0;32m--> 391\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_FuncPtr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_ordinal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name_or_ordinal, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    393\u001b[0m         func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m name_or_ordinal\n",
      "\u001b[0;31mAttributeError\u001b[0m: ./libc_module.so: undefined symbol: calc"
     ]
    }
   ],
   "source": [
    "import ctypes                           # 파이썬 extension을 사용하기 위한 모듈\n",
    "import platform                         # 파이썬 아키텍처를 확인하기 위한 모듈\n",
    "\n",
    "path = \"./libc_module.so\"\n",
    "c_module = ctypes.cdll.LoadLibrary(path)\n",
    "\n",
    "comm = c_module.calc\n",
    "comm.restype = None\n",
    "comm()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
